# üÉè Reinforcement Learning BlackJack Player

**Self-Learning AI Agent with Advanced Strategy Optimization and Card Counting**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Reinforcement Learning](https://img.shields.io/badge/RL-Q--Learning-red.svg)]()
[![Game Theory](https://img.shields.io/badge/Game%20Theory-Strategy%20Optimization-green.svg)]()
[![Research Paper](https://img.shields.io/badge/IEEE-Conference%20Paper-orange.svg)]()
[![University Project](https://img.shields.io/badge/University-THWS-purple.svg)](https://www.thws.de/)

> **Academic Research Project**: Reasoning and Decision Making under Uncertainty  
> **Institution**: Technical University of Applied Sciences W√ºrzburg-Schweinfurt (THWS)  
> **Course**: Prof. Dr. Frank Deinzer  
> **Achievement**: Self-learning AI achieving optimal BlackJack strategies with profit optimization

## üéØ Project Overview

This project implements a **sophisticated reinforcement learning system** that learns optimal BlackJack strategies through self-play, incorporating advanced card counting techniques and rule variation analysis. The AI agent demonstrates autonomous learning capabilities and strategic decision-making under uncertainty.

### üèÜ Key Achievements
- **Self-Learning AI Agent**: Autonomous strategy development through reinforcement learning
- **Advanced Card Counting**: Implementation of Complete Point-Count System from Edward Thorp's "Beat the Dealer"
- **Strategy Optimization**: Profit maximization through policy refinement and exploration  
- **Rule Variation Analysis**: Adaptive strategies for different BlackJack game variants
- **Academic Publication**: IEEE conference paper with comprehensive experimental analysis
- **Custom RL Implementation**: No external frameworks - pure algorithmic implementation

## üî¨ Technical Implementation

### **Research Tasks Completed**

#### **Task P3.1: Reinforcement Learning Implementation**
1. **Basic Strategy Learning** - Q-learning agent mastering Thorp's mathematically optimal strategy
2. **Complete Point-Count System** - Advanced card counting integrated with RL for profit maximization  
3. **Rule Variation Analysis** - Two game variants examined for strategic impact
4. **Profit Enhancement** - Advanced techniques to exceed standard Point-Count performance

#### **Task P3.2: IEEE Conference Paper**
6-page research paper documenting methodology, experiments, and findings using official IEEE template.

### **Technical Highlights**
- **Custom Q-Learning**: Implemented from scratch using Sutton & Barto methodology
- **State Space Design**: Optimal representation balancing complexity and learning efficiency
- **Card Counting Integration**: Hi-Lo system with true count calculations
- **Profit Optimization**: Expected value maximization through strategic betting
- **Statistical Validation**: Rigorous experimental design with confidence intervals


### **Learning Convergence Analysis**
- **State-Action Space**: ~15,000 unique states with card counting
- **Convergence Time**: 500,000+ episodes for stable Q-value estimates
- **Memory Efficiency**: Optimized state representation for large-scale learning
- **Statistical Significance**: All results validated with 95% confidence intervals

## üöÄ Getting Started

### **Prerequisites**
```bash
Python 3.8+
NumPy >= 1.21.0
Matplotlib >= 3.4.0
Pandas >= 1.3.0
Jupyter >= 1.0.0


## üî¨ Research Contributions

### **Scientific Questions Addressed**
1. **Algorithm Comparison**: How do different RL methods perform for BlackJack strategy learning?
2. **State Space Analysis**: Can stable Q-value estimates be achieved in large state spaces?
3. **Rule Impact**: How do rule variations affect learned optimal policies?
4. **Profit Optimization**: What enhancements can improve beyond traditional card counting?

### **Key Findings**
- **Enhanced Point-Count System** achieves 2.1% expected value (vs. -0.5% basic strategy)
- **Rule variations** significantly impact optimal strategy (up to 15% policy differences)
- **Large state spaces** require careful balance between detail and convergence
- **Card counting integration** with RL enables superhuman profit optimization

## üéì Skills Demonstrated

**Reinforcement Learning:**
- Custom Q-learning implementation without external frameworks
- Large state-space handling and function approximation
- Exploration-exploitation balance optimization
- Multi-strategy comparison and evaluation

**Mathematical Modeling:**
- Markov Decision Process formulation for card games
- Statistical analysis and hypothesis testing
- Probability theory and expected value calculations  
- Game theory and optimal strategy derivation

**Academic Research:**
- IEEE-standard conference paper writing
- Experimental design and statistical validation
- Literature review and theoretical grounding
- Reproducible research with comprehensive documentation

## üìß Contact

**Gurudeep Haleangadi Nagesh**  
Master's Student in Artificial Intelligence  
Technical University of Applied Sciences W√ºrzburg-Schweinfurt

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://linkedin.com/in/gurudeephn)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-black)](https://github.com/Gurudeep-hn)
[![Email](https://img.shields.io/badge/Email-Contact-red)](mailto:gurudeep409@gmail.com)

---

**‚≠ê If you found this reinforcement learning implementation valuable, please give it a star!**

*This project demonstrates advanced RL techniques for strategic optimization under uncertainty, with applications in finance, gaming, and autonomous decision-making systems.*
```
